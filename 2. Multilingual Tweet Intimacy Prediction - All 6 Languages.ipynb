{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "661e6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#Preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Models from sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a71fc",
   "metadata": {},
   "source": [
    "#### Data read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b439a25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label language\n",
      "0     wearing a fake engagement ring so guys won’t a...    1.8  English\n",
      "1                                  Bees vs. Wasps. http    1.0  English\n",
      "2                  Here is a nice equation: 0+0-0-0+0=0    1.0  English\n",
      "3                  @user @user Enjoy each new day!😊🇨🇦🐞🐭    1.6  English\n",
      "4     I can be having a perfectly good day then I th...    1.6  English\n",
      "...                                                 ...    ...      ...\n",
      "9486  若被確認為「國際關注公共衛生緊急事件」， 世衛會發布一系列包括確診、隔離和治療的詳細計畫， ...    1.0  Chinese\n",
      "9487                                @user 是嗎？ 可能我沒有注意到吧    2.0  Chinese\n",
      "9488                                 @user @user 你剃过毛毛吗    3.8  Chinese\n",
      "9489                                      @user 她没说是捐吧？    1.8  Chinese\n",
      "9490  通报来了 真的要消停一会了 视频不要私信要啦 就当2w粉的福利提前放出来吧 有风险勿模仿 感...    1.6  Chinese\n",
      "\n",
      "[9491 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_all = pd.read_csv('train.csv')\n",
    "print(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921f426",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7816afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = re.sub('@user', '', text)\n",
    "    text = re.sub('http', '', text)\n",
    "    text = re.sub('@[\\w]+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#data_all['text'] = data_all['text'].apply(clean)\n",
    "\n",
    "#data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54c788",
   "metadata": {},
   "source": [
    "#### Tokenizing and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bf0e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data_all['text'] = data_all['text'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba28df",
   "metadata": {},
   "source": [
    "#### Creating datasets by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebff88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = data_all[data_all['language'] == 'English'] \n",
    "chinese = data_all[data_all['language'] == 'Chinese'] \n",
    "french = data_all[data_all['language'] == 'French'] \n",
    "italian = data_all[data_all['language'] == 'Italian'] \n",
    "portuguese = data_all[data_all['language'] == 'Portuguese'] \n",
    "spanish = data_all[data_all['language'] == 'Spanish'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecedf40",
   "metadata": {},
   "source": [
    "#### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label language\n",
      "0  wearing a fake engagement ring so guy won ’ t ...    1.8  English\n",
      "1                              Bees vs. Wasps . http    1.0  English\n",
      "2              Here is a nice equation : 0+0-0-0+0=0    1.0  English\n",
      "3           @ user @ user Enjoy each new day ! 😊🇨🇦🐞🐭    1.6  English\n",
      "4  I can be having a perfectly good day then I th...    1.6  English\n",
      "Num rows: 9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_chinese = set(stopwords.words('chinese'))\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "stop_words_italian = set(stopwords.words('italian'))\n",
    "stop_words_portuguese = set(stopwords.words('portuguese'))\n",
    "stop_words_spanish = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_english(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_english]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#english['text'] = english['text'].apply(remove_stopwords_english)\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_chinese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_chinese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#chinese['text'] = chinese['text'].apply(remove_stopwords_chinese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_french(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_french]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#french['text'] = french['text'].apply(remove_stopwords_french)\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_italian(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_italian]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#italian['text'] = italian['text'].apply(remove_stopwords_italian)\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_portuguese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_portuguese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#portuguese['text'] = portuguese['text'].apply(remove_stopwords_portuguese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_spanish(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_spanish]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#spanish['text'] = spanish['text'].apply(remove_stopwords_spanish)\n",
    "\n",
    "\n",
    "\n",
    "#data  = pd.concat([english, chinese, french, italian, portuguese, spanish])\n",
    "data = data_all\n",
    "\n",
    "print(data.head())\n",
    "print(\"Num rows:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4e202",
   "metadata": {},
   "source": [
    "#### Feature extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bc139fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data['text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414c844",
   "metadata": {},
   "source": [
    "#### Tran and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e03498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2a06",
   "metadata": {},
   "source": [
    "#### Creating a function to predict and evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc6f51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(mod, X_tr, y_tr, X_ts, y_ts):\n",
    "    model = mod\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_ts)\n",
    "    \n",
    "    pearson_r, _ = pearsonr(y_pred, y_ts)\n",
    "    print(\"Pearson's r for\", model, \"is: \" , pearson_r)\n",
    "    \n",
    "    def calculate_pearson(mod, X_ts, y_ts):\n",
    "        y_pred_1 = mod.predict(X_ts)\n",
    "        pearson, _ = pearsonr(y_pred_1, y_ts)\n",
    "        return pearson\n",
    "    \n",
    "    pearson_cv = cross_val_score(model, X_ts, y_ts, scoring=calculate_pearson, cv=5)\n",
    "    print(\"Pearson's r for\", model, \"after cross validation is: \" , pearson_cv)\n",
    "    \n",
    "    mse = mean_squared_error(y_pred, y_ts)\n",
    "    print(\"Mean Square Error for\", model, \"is: \" , mse)\n",
    "      \n",
    "    mse_cv = -cross_val_score(model, X_ts, y_ts, scoring='neg_mean_squared_error', cv=5)\n",
    "    print(\"Mean Square Error for\", model, \"after cross validation is: \" , mse_cv)\n",
    "    \n",
    "    return pearson_r, pearson_cv, mse, mse_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94911ec6",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f3acef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for LinearRegression() is:  0.23089700589113848\n",
      "Pearson's r for LinearRegression() after cross validation is:  [0.22241299 0.29895702 0.25895541 0.27400264 0.31028882]\n",
      "Mean Square Error for LinearRegression() is:  1.077527942861177\n",
      "Mean Square Error for LinearRegression() after cross validation is:  [0.72403087 0.81785748 0.71789167 0.73061872 0.81930056]\n"
     ]
    }
   ],
   "source": [
    "lr = models(linear_model.LinearRegression(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f8762",
   "metadata": {},
   "source": [
    "#### Service Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d78aa19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for SVR() is:  0.4127894693459429\n",
      "Pearson's r for SVR() after cross validation is:  [0.30831422 0.30930053 0.29001737 0.34050096 0.3746493 ]\n",
      "Mean Square Error for SVR() is:  0.6670610736297159\n",
      "Mean Square Error for SVR() after cross validation is:  [0.6592582  0.79862815 0.68062319 0.69242026 0.8211113 ]\n"
     ]
    }
   ],
   "source": [
    "svr = models(svm.SVR(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689029d",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b86d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for DecisionTreeRegressor() is:  0.21840287370360903\n",
      "Pearson's r for DecisionTreeRegressor() after cross validation is:  [0.20424259 0.21834384 0.18292873 0.24837325 0.23768277]\n",
      "Mean Square Error for DecisionTreeRegressor() is:  1.1645770473208241\n",
      "Mean Square Error for DecisionTreeRegressor() after cross validation is:  [1.2146531  1.46867333 1.23176543 1.25405482 1.36521841]\n"
     ]
    }
   ],
   "source": [
    "dt = models(tree.DecisionTreeRegressor(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691706b",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0af13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for Ridge(alpha=0.9) is:  0.41045168257446313\n",
      "Pearson's r for Ridge(alpha=0.9) after cross validation is:  [0.28939259 0.31244344 0.29504246 0.34804303 0.36689302]\n",
      "Mean Square Error for Ridge(alpha=0.9) is:  0.6613361135725191\n",
      "Mean Square Error for Ridge(alpha=0.9) after cross validation is:  [0.66303318 0.7829483  0.66233566 0.67191569 0.78456122]\n"
     ]
    }
   ],
   "source": [
    "ridge = models(linear_model.Ridge(alpha=0.9), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e537af",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e52ae765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for RandomForestRegressor(random_state=42) is:  0.3438763305236285\n",
      "Pearson's r for RandomForestRegressor(random_state=42) after cross validation is:  [0.24358303 0.21107175 0.26506125 0.33971407 0.33833686]\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) is:  0.7570275702431948\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) after cross validation is:  [0.81503506 1.01898314 0.83333417 0.77170308 0.94353945]\n"
     ]
    }
   ],
   "source": [
    "rf = models(RandomForestRegressor(random_state=42), X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
