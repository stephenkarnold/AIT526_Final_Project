{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661e6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenarnoldkappala/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#Preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Models from sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a71fc",
   "metadata": {},
   "source": [
    "#### Data read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b439a25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label language\n",
      "0     wearing a fake engagement ring so guys won’t a...    1.8  English\n",
      "1                                  Bees vs. Wasps. http    1.0  English\n",
      "2                  Here is a nice equation: 0+0-0-0+0=0    1.0  English\n",
      "3                  @user @user Enjoy each new day!😊🇨🇦🐞🐭    1.6  English\n",
      "4     I can be having a perfectly good day then I th...    1.6  English\n",
      "...                                                 ...    ...      ...\n",
      "9486  若被確認為「國際關注公共衛生緊急事件」， 世衛會發布一系列包括確診、隔離和治療的詳細計畫， ...    1.0  Chinese\n",
      "9487                                @user 是嗎？ 可能我沒有注意到吧    2.0  Chinese\n",
      "9488                                 @user @user 你剃过毛毛吗    3.8  Chinese\n",
      "9489                                      @user 她没说是捐吧？    1.8  Chinese\n",
      "9490  通报来了 真的要消停一会了 视频不要私信要啦 就当2w粉的福利提前放出来吧 有风险勿模仿 感...    1.6  Chinese\n",
      "\n",
      "[9491 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_all = pd.read_csv('train.csv')\n",
    "print(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921f426",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7816afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = re.sub('@user', '', text)\n",
    "    text = re.sub('http', '', text)\n",
    "    text = re.sub('@[\\w]+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#data_all['text'] = data_all['text'].apply(clean)\n",
    "\n",
    "#data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54c788",
   "metadata": {},
   "source": [
    "#### Tokenizing and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf0e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data_all['text'] = data_all['text'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba28df",
   "metadata": {},
   "source": [
    "#### Creating datasets by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebff88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = data_all[data_all['language'] == 'English'] \n",
    "chinese = data_all[data_all['language'] == 'Chinese'] \n",
    "french = data_all[data_all['language'] == 'French'] \n",
    "italian = data_all[data_all['language'] == 'Italian'] \n",
    "portuguese = data_all[data_all['language'] == 'Portuguese'] \n",
    "spanish = data_all[data_all['language'] == 'Spanish'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecedf40",
   "metadata": {},
   "source": [
    "#### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label language\n",
      "0  wearing a fake engagement ring so guy won ’ t ...    1.8  English\n",
      "1                              Bees vs. Wasps . http    1.0  English\n",
      "2              Here is a nice equation : 0+0-0-0+0=0    1.0  English\n",
      "3           @ user @ user Enjoy each new day ! 😊🇨🇦🐞🐭    1.6  English\n",
      "4  I can be having a perfectly good day then I th...    1.6  English\n",
      "Num rows: 9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_chinese = set(stopwords.words('chinese'))\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "stop_words_italian = set(stopwords.words('italian'))\n",
    "stop_words_portuguese = set(stopwords.words('portuguese'))\n",
    "stop_words_spanish = set(stopwords.words('spanish'))\n",
    "stop_words_arabic = set(stopwords.words('arabic'))\n",
    "stop_words_dutch = set(stopwords.words('dutch'))\n",
    "#stop_words_hindi = set(stopwords.words('hindi'))\n",
    "#stop_words_korean = set(stopwords.words('korean'))\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_english(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_english]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#english['text'] = english['text'].apply(remove_stopwords_english)\n",
    "\n",
    "\n",
    "# Function to remove stop words for chinese\n",
    "def remove_stopwords_chinese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_chinese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#chinese['text'] = chinese['text'].apply(remove_stopwords_chinese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for french\n",
    "def remove_stopwords_french(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_french]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#french['text'] = french['text'].apply(remove_stopwords_french)\n",
    "\n",
    "\n",
    "# Function to remove stop words for italian\n",
    "def remove_stopwords_italian(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_italian]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#italian['text'] = italian['text'].apply(remove_stopwords_italian)\n",
    "\n",
    "\n",
    "# Function to remove stop words for portuguese\n",
    "def remove_stopwords_portuguese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_portuguese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#portuguese['text'] = portuguese['text'].apply(remove_stopwords_portuguese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for spanish\n",
    "def remove_stopwords_spanish(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_spanish]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#spanish['text'] = spanish['text'].apply(remove_stopwords_spanish)\n",
    "\n",
    "# Function to remove stop words for arabic\n",
    "def remove_stopwords_arabic(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_arabic]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for dutch\n",
    "def remove_stopwords_dutch(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_dutch]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for hindi\n",
    "#def remove_stopwords_hindi(text):\n",
    "    \n",
    "#    words = word_tokenize(text)\n",
    "#    filtered_words = [word for word in words if word.lower() not in stop_words_hindi]\n",
    "#    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for korean\n",
    "#def remove_stopwords_korean(text):\n",
    "    \n",
    "#    words = word_tokenize(text)\n",
    "#    filtered_words = [word for word in words if word.lower() not in stop_words_korean]\n",
    "#    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "#data  = pd.concat([english, chinese, french, italian, portuguese, spanish])\n",
    "data = data_all\n",
    "\n",
    "print(data.head())\n",
    "print(\"Num rows:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4e202",
   "metadata": {},
   "source": [
    "#### Feature extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc139fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data['text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414c844",
   "metadata": {},
   "source": [
    "#### Tran and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2a06",
   "metadata": {},
   "source": [
    "#### Creating a function to predict and evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6f51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(mod, X_tr, y_tr, X_ts, y_ts):\n",
    "    model = mod\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_ts)\n",
    "    \n",
    "    pearson_r, _ = pearsonr(y_pred, y_ts)\n",
    "    print(\"Pearson's r for\", model, \"is: \" , pearson_r)\n",
    "    \n",
    "    def calculate_pearson(mod, X_ts, y_ts):\n",
    "        y_pred_1 = mod.predict(X_ts)\n",
    "        pearson, _ = pearsonr(y_pred_1, y_ts)\n",
    "        return pearson\n",
    "    \n",
    "    pearson_cv = cross_val_score(model, X_ts, y_ts, scoring=calculate_pearson, cv=5)\n",
    "    print(\"Pearson's r for\", model, \"after cross validation is: \" , pearson_cv)\n",
    "    \n",
    "    mse = mean_squared_error(y_pred, y_ts)\n",
    "    print(\"Mean Square Error for\", model, \"is: \" , mse)\n",
    "      \n",
    "    mse_cv = -cross_val_score(model, X_ts, y_ts, scoring='neg_mean_squared_error', cv=5)\n",
    "    print(\"Mean Square Error for\", model, \"after cross validation is: \" , mse_cv)\n",
    "    \n",
    "    return pearson_r, pearson_cv, mse, mse_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94911ec6",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3acef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for LinearRegression() is:  0.23089700589113848\n",
      "Pearson's r for LinearRegression() after cross validation is:  [0.22241299 0.29895702 0.25895541 0.27400264 0.31028882]\n",
      "Mean Square Error for LinearRegression() is:  1.077527942861177\n",
      "Mean Square Error for LinearRegression() after cross validation is:  [0.72403087 0.81785748 0.71789167 0.73061872 0.81930056]\n"
     ]
    }
   ],
   "source": [
    "lr = models(linear_model.LinearRegression(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f8762",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78aa19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for SVR() is:  0.4127894693459429\n",
      "Pearson's r for SVR() after cross validation is:  [0.30831422 0.30930053 0.29001737 0.34050096 0.3746493 ]\n",
      "Mean Square Error for SVR() is:  0.6670610736297159\n",
      "Mean Square Error for SVR() after cross validation is:  [0.6592582  0.79862815 0.68062319 0.69242026 0.8211113 ]\n"
     ]
    }
   ],
   "source": [
    "svr = models(svm.SVR(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689029d",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b86d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for DecisionTreeRegressor() is:  0.20263964456394604\n",
      "Pearson's r for DecisionTreeRegressor() after cross validation is:  [0.21787455 0.16428198 0.20434918 0.24173278 0.22503976]\n",
      "Mean Square Error for DecisionTreeRegressor() is:  1.1960754602282024\n",
      "Mean Square Error for DecisionTreeRegressor() after cross validation is:  [1.11076202 1.45642333 1.14555344 1.17634284 1.32125916]\n"
     ]
    }
   ],
   "source": [
    "dt = models(tree.DecisionTreeRegressor(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691706b",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0af13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for Ridge(alpha=0.9) is:  0.41045168257446313\n",
      "Pearson's r for Ridge(alpha=0.9) after cross validation is:  [0.28939259 0.31244344 0.29504246 0.34804303 0.36689302]\n",
      "Mean Square Error for Ridge(alpha=0.9) is:  0.6613361135725191\n",
      "Mean Square Error for Ridge(alpha=0.9) after cross validation is:  [0.66303318 0.7829483  0.66233566 0.67191569 0.78456122]\n"
     ]
    }
   ],
   "source": [
    "ridge = models(linear_model.Ridge(alpha=0.9), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e537af",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52ae765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for RandomForestRegressor(random_state=42) is:  0.3438763305236285\n",
      "Pearson's r for RandomForestRegressor(random_state=42) after cross validation is:  [0.24358303 0.21107175 0.26506125 0.33971407 0.33833686]\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) is:  0.7570275702431948\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) after cross validation is:  [0.81503506 1.01898314 0.83333417 0.77170308 0.94353945]\n"
     ]
    }
   ],
   "source": [
    "rf = models(RandomForestRegressor(random_state=42), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5012323",
   "metadata": {},
   "source": [
    "# Evaluation on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228c4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label    language\n",
      "0                                             @user 아..    3.0      Korean\n",
      "1                                @user @user je rêve ??    2.2      French\n",
      "2                                          thank u, nxt    1.0     English\n",
      "3                            @user ma che cosa HO FATTO    2.6     Italian\n",
      "4      在教室打飞机，站累了，就搬凳子坐下，站着坐着都是你爸爸，听爸爸的话哦！骚货，爸爸爱你。 http    3.0     Chinese\n",
      "...                                                 ...    ...         ...\n",
      "3876  @user Não sei se ele vai terminar com a Kyra (...    3.0  Portuguese\n",
      "3877  @user Coitada...fraquinha....Povo precisa sabe...    1.4  Portuguese\n",
      "3878  @user أي عطر حبيبي .... مساعدة بشار البعثي عطر ؟؟    3.5      Arabic\n",
      "3879  TSC promoveert naar tweede klasse, ontgoocheli...    1.0       Dutch\n",
      "3880  @user if you pull up to emmas house bald tomor...    1.8     English\n",
      "\n",
      "[3881 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d3b896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "#test_data['text'] = test_data['text'].apply(clean)\n",
    "\n",
    "# Tokenizing and Lemmatization\n",
    "test_data['text'] = test_data['text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "#Creating datasets by language\n",
    "english_test = test_data[test_data['language'] == 'English'] \n",
    "chinese_test = test_data[test_data['language'] == 'Chinese'] \n",
    "french_test = test_data[test_data['language'] == 'French'] \n",
    "italian_test = test_data[test_data['language'] == 'Italian'] \n",
    "portuguese_test = test_data[test_data['language'] == 'Portuguese'] \n",
    "spanish_test = test_data[test_data['language'] == 'Spanish'] \n",
    "arabic_test = test_data[test_data['language'] == 'Arabic'] \n",
    "dutch_test = test_data[test_data['language'] == 'Dutch'] \n",
    "hindi_test = test_data[test_data['language'] == 'Hindi'] \n",
    "korean_test = test_data[test_data['language'] == 'Korean'] \n",
    "\n",
    "# Removing Stopwords\n",
    "#english_test['text'] = english_test['text'].apply(remove_stopwords_english)\n",
    "#chinese_test['text'] = chinese_test['text'].apply(remove_stopwords_chinese)\n",
    "#french_test['text'] = french_test['text'].apply(remove_stopwords_french)\n",
    "#italian_test['text'] = italian_test['text'].apply(remove_stopwords_italian)\n",
    "#portuguese_test['text'] = portuguese_test['text'].apply(remove_stopwords_portuguese)\n",
    "#spanish_test['text'] = spanish_test['text'].apply(remove_stopwords_spanish)\n",
    "#arabic_test['text'] = arabic_test['text'].apply(remove_stopwords_arabic)\n",
    "#dutch_test['text'] = dutch_test['text'].apply(remove_stopwords_dutch)\n",
    "#hindi_test['text'] = hindi_test['text'].apply(remove_stopwords_hindi)\n",
    "#korean_test['text'] = korean_test['text'].apply(remove_stopwords_korean)\n",
    "\n",
    "#test  = pd.concat([english_test, chinese_test, french_test, italian_test, portuguese_test, spanish_test, arabic_test, dutch_test, hindi_test, korean_test])\n",
    "test = test_data\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "test_X = tfidf_vectorizer.transform(test['text'])\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eae870",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a82fbe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for LinearRegression() is:  0.1466130222052112\n",
      "Pearson's r for LinearRegression() after cross validation is:  [0.20851078 0.16861297 0.18241796 0.22750515 0.19299588]\n",
      "Mean Square Error for LinearRegression() is:  1.4928472459008737\n",
      "Mean Square Error for LinearRegression() after cross validation is:  [1.15868499 1.23329957 1.10916381 1.06707512 1.25833742]\n"
     ]
    }
   ],
   "source": [
    "lr_test = models(linear_model.LinearRegression(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aaa28",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f15efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for SVR() is:  0.2298459746013823\n",
      "Pearson's r for SVR() after cross validation is:  [0.35319267 0.31038057 0.33721598 0.32290689 0.3306959 ]\n",
      "Mean Square Error for SVR() is:  0.9749033682013704\n",
      "Mean Square Error for SVR() after cross validation is:  [0.88968995 0.89137452 0.80110303 0.8175126  0.84275587]\n"
     ]
    }
   ],
   "source": [
    "svr_test = models(svm.SVR(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9f45e",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b54685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for DecisionTreeRegressor() is:  0.18205010322063803\n",
      "Pearson's r for DecisionTreeRegressor() after cross validation is:  [0.15654114 0.16338708 0.21649265 0.19948342 0.19191758]\n",
      "Mean Square Error for DecisionTreeRegressor() is:  1.2375066454803945\n",
      "Mean Square Error for DecisionTreeRegressor() after cross validation is:  [1.2262826  1.25483461 1.13938939 1.14244206 1.19548184]\n"
     ]
    }
   ],
   "source": [
    "dt_test = models(tree.DecisionTreeRegressor(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09127f37",
   "metadata": {},
   "source": [
    "#### Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8b5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for Ridge(alpha=0.9) is:  0.21784341254799272\n",
      "Pearson's r for Ridge(alpha=0.9) after cross validation is:  [0.29606818 0.27051856 0.29168931 0.28057036 0.28893488]\n",
      "Mean Square Error for Ridge(alpha=0.9) is:  1.018905446864162\n",
      "Mean Square Error for Ridge(alpha=0.9) after cross validation is:  [0.88314438 0.89725691 0.824735   0.83063461 0.85828462]\n"
     ]
    }
   ],
   "source": [
    "ridge_test = models(linear_model.Ridge(alpha=0.9), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd18a9",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a8377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for RandomForestRegressor(random_state=42) is:  0.26227367783835914\n",
      "Pearson's r for RandomForestRegressor(random_state=42) after cross validation is:  [0.28657197 0.27730172 0.28656303 0.30693723 0.22101396]\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) is:  0.9595807549608741\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) after cross validation is:  [0.92961157 0.94451142 0.85728764 0.85166243 0.94838757]\n"
     ]
    }
   ],
   "source": [
    "rf_test = models(RandomForestRegressor(random_state=42), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5af4fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_X)\n",
    "    \n",
    "test_with_pred = pd.DataFrame({\n",
    "        'text': test['text'],\n",
    "        'language': test['language'],\n",
    "        'predicted_label': y_pred,\n",
    "        'true_label': test_y\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095dfe99",
   "metadata": {},
   "source": [
    "#### Pearson R for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb99615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for Korean with SVR is -0.04630847800407837\n",
      "Pearson correlation for Arabic with SVR is 0.1715946218286482\n",
      "Pearson correlation for Portuguese with SVR is 0.44983401762435216\n",
      "Pearson correlation for English with SVR is 0.40963944670035474\n",
      "Pearson correlation for French with SVR is 0.45503915202838935\n",
      "Pearson correlation for Spanish with SVR is 0.5242129956623359\n",
      "Pearson correlation for Italian with SVR is 0.416692357071758\n",
      "Pearson correlation for Hindi with SVR is 0.17567025158467825\n",
      "Pearson correlation for Dutch with SVR is 0.19130633387004328\n",
      "Pearson correlation for Chinese with SVR is -0.005174803024200055\n"
     ]
    }
   ],
   "source": [
    "def calculate_pearsonr_for_diff_lang(language, data):\n",
    "    language_with_pred = test_with_pred[test_with_pred['language'] == language]\n",
    "    pearson, _ = pearsonr(language_with_pred['predicted_label'], language_with_pred['true_label'])\n",
    "    \n",
    "    return pearson\n",
    "\n",
    "# Example usage:\n",
    "languages = set(test_with_pred['language'])\n",
    "\n",
    "for lang in languages:\n",
    "    correlation = calculate_pearsonr_for_diff_lang(lang, test_with_pred)\n",
    "    print(f\"Pearson correlation for {lang} with SVR is {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f13e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
