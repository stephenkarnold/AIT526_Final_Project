{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661e6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenarnoldkappala/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#Preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Models from sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a71fc",
   "metadata": {},
   "source": [
    "#### Data read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b439a25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label language\n",
      "0     wearing a fake engagement ring so guys wonâ€™t a...    1.8  English\n",
      "1                                  Bees vs. Wasps. http    1.0  English\n",
      "2                  Here is a nice equation: 0+0-0-0+0=0    1.0  English\n",
      "3                  @user @user Enjoy each new day!ğŸ˜ŠğŸ‡¨ğŸ‡¦ğŸğŸ­    1.6  English\n",
      "4     I can be having a perfectly good day then I th...    1.6  English\n",
      "...                                                 ...    ...      ...\n",
      "9486  è‹¥è¢«ç¢ºèªç‚ºã€Œåœ‹éš›é—œæ³¨å…¬å…±è¡›ç”Ÿç·Šæ€¥äº‹ä»¶ã€ï¼Œ ä¸–è¡›æœƒç™¼å¸ƒä¸€ç³»åˆ—åŒ…æ‹¬ç¢ºè¨ºã€éš”é›¢å’Œæ²»ç™‚çš„è©³ç´°è¨ˆç•«ï¼Œ ...    1.0  Chinese\n",
      "9487                                @user æ˜¯å—ï¼Ÿ å¯èƒ½æˆ‘æ²’æœ‰æ³¨æ„åˆ°å§    2.0  Chinese\n",
      "9488                                 @user @user ä½ å‰ƒè¿‡æ¯›æ¯›å—    3.8  Chinese\n",
      "9489                                      @user å¥¹æ²¡è¯´æ˜¯æå§ï¼Ÿ    1.8  Chinese\n",
      "9490  é€šæŠ¥æ¥äº† çœŸçš„è¦æ¶ˆåœä¸€ä¼šäº† è§†é¢‘ä¸è¦ç§ä¿¡è¦å•¦ å°±å½“2wç²‰çš„ç¦åˆ©æå‰æ”¾å‡ºæ¥å§ æœ‰é£é™©å‹¿æ¨¡ä»¿ æ„Ÿ...    1.6  Chinese\n",
      "\n",
      "[9491 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_all = pd.read_csv('train.csv')\n",
    "print(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921f426",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7816afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = re.sub('@user', '', text)\n",
    "    text = re.sub('http', '', text)\n",
    "    text = re.sub('@[\\w]+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#data_all['text'] = data_all['text'].apply(clean)\n",
    "\n",
    "#data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54c788",
   "metadata": {},
   "source": [
    "#### Tokenizing and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf0e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data_all['text'] = data_all['text'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba28df",
   "metadata": {},
   "source": [
    "#### Creating datasets by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebff88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = data_all[data_all['language'] == 'English'] \n",
    "chinese = data_all[data_all['language'] == 'Chinese'] \n",
    "french = data_all[data_all['language'] == 'French'] \n",
    "italian = data_all[data_all['language'] == 'Italian'] \n",
    "portuguese = data_all[data_all['language'] == 'Portuguese'] \n",
    "spanish = data_all[data_all['language'] == 'Spanish'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecedf40",
   "metadata": {},
   "source": [
    "#### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e914282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label language\n",
      "0  wearing a fake engagement ring so guy won â€™ t ...    1.8  English\n",
      "1                              Bees vs. Wasps . http    1.0  English\n",
      "2              Here is a nice equation : 0+0-0-0+0=0    1.0  English\n",
      "3           @ user @ user Enjoy each new day ! ğŸ˜ŠğŸ‡¨ğŸ‡¦ğŸğŸ­    1.6  English\n",
      "4  I can be having a perfectly good day then I th...    1.6  English\n",
      "Num rows: 9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stephenarnoldkappala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_chinese = set(stopwords.words('chinese'))\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "stop_words_italian = set(stopwords.words('italian'))\n",
    "stop_words_portuguese = set(stopwords.words('portuguese'))\n",
    "stop_words_spanish = set(stopwords.words('spanish'))\n",
    "stop_words_arabic = set(stopwords.words('arabic'))\n",
    "stop_words_dutch = set(stopwords.words('dutch'))\n",
    "#stop_words_hindi = set(stopwords.words('hindi'))\n",
    "#stop_words_korean = set(stopwords.words('korean'))\n",
    "\n",
    "\n",
    "# Function to remove stop words for english\n",
    "def remove_stopwords_english(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_english]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#english['text'] = english['text'].apply(remove_stopwords_english)\n",
    "\n",
    "\n",
    "# Function to remove stop words for chinese\n",
    "def remove_stopwords_chinese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_chinese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#chinese['text'] = chinese['text'].apply(remove_stopwords_chinese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for french\n",
    "def remove_stopwords_french(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_french]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#french['text'] = french['text'].apply(remove_stopwords_french)\n",
    "\n",
    "\n",
    "# Function to remove stop words for italian\n",
    "def remove_stopwords_italian(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_italian]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#italian['text'] = italian['text'].apply(remove_stopwords_italian)\n",
    "\n",
    "\n",
    "# Function to remove stop words for portuguese\n",
    "def remove_stopwords_portuguese(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_portuguese]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#portuguese['text'] = portuguese['text'].apply(remove_stopwords_portuguese)\n",
    "\n",
    "\n",
    "# Function to remove stop words for spanish\n",
    "def remove_stopwords_spanish(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_spanish]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#spanish['text'] = spanish['text'].apply(remove_stopwords_spanish)\n",
    "\n",
    "# Function to remove stop words for arabic\n",
    "def remove_stopwords_arabic(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_arabic]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for dutch\n",
    "def remove_stopwords_dutch(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_dutch]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for hindi\n",
    "#def remove_stopwords_hindi(text):\n",
    "    \n",
    "#    words = word_tokenize(text)\n",
    "#    filtered_words = [word for word in words if word.lower() not in stop_words_hindi]\n",
    "#    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to remove stop words for korean\n",
    "#def remove_stopwords_korean(text):\n",
    "    \n",
    "#    words = word_tokenize(text)\n",
    "#    filtered_words = [word for word in words if word.lower() not in stop_words_korean]\n",
    "#    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "#data  = pd.concat([english, chinese, french, italian, portuguese, spanish])\n",
    "data = data_all\n",
    "\n",
    "print(data.head())\n",
    "print(\"Num rows:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4e202",
   "metadata": {},
   "source": [
    "#### Feature extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc139fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data['text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414c844",
   "metadata": {},
   "source": [
    "#### Tran and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2a06",
   "metadata": {},
   "source": [
    "#### Creating a function to predict and evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6f51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(mod, X_tr, y_tr, X_ts, y_ts):\n",
    "    model = mod\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_ts)\n",
    "    \n",
    "    pearson_r, _ = pearsonr(y_pred, y_ts)\n",
    "    print(\"Pearson's r for\", model, \"is: \" , pearson_r)\n",
    "    \n",
    "    def calculate_pearson(mod, X_ts, y_ts):\n",
    "        y_pred_1 = mod.predict(X_ts)\n",
    "        pearson, _ = pearsonr(y_pred_1, y_ts)\n",
    "        return pearson\n",
    "    \n",
    "    pearson_cv = cross_val_score(model, X_ts, y_ts, scoring=calculate_pearson, cv=5)\n",
    "    print(\"Pearson's r for\", model, \"after cross validation is: \" , pearson_cv)\n",
    "    \n",
    "    mse = mean_squared_error(y_pred, y_ts)\n",
    "    print(\"Mean Square Error for\", model, \"is: \" , mse)\n",
    "      \n",
    "    mse_cv = -cross_val_score(model, X_ts, y_ts, scoring='neg_mean_squared_error', cv=5)\n",
    "    print(\"Mean Square Error for\", model, \"after cross validation is: \" , mse_cv)\n",
    "    \n",
    "    return pearson_r, pearson_cv, mse, mse_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94911ec6",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3acef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for LinearRegression() is:  0.23089700589113848\n",
      "Pearson's r for LinearRegression() after cross validation is:  [0.22241299 0.29895702 0.25895541 0.27400264 0.31028882]\n",
      "Mean Square Error for LinearRegression() is:  1.077527942861177\n",
      "Mean Square Error for LinearRegression() after cross validation is:  [0.72403087 0.81785748 0.71789167 0.73061872 0.81930056]\n"
     ]
    }
   ],
   "source": [
    "lr = models(linear_model.LinearRegression(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f8762",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78aa19b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for SVR() is:  0.4127894693459429\n",
      "Pearson's r for SVR() after cross validation is:  [0.30831422 0.30930053 0.29001737 0.34050096 0.3746493 ]\n",
      "Mean Square Error for SVR() is:  0.6670610736297159\n",
      "Mean Square Error for SVR() after cross validation is:  [0.6592582  0.79862815 0.68062319 0.69242026 0.8211113 ]\n"
     ]
    }
   ],
   "source": [
    "svr = models(svm.SVR(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689029d",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b86d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for DecisionTreeRegressor() is:  0.20263964456394604\n",
      "Pearson's r for DecisionTreeRegressor() after cross validation is:  [0.21787455 0.16428198 0.20434918 0.24173278 0.22503976]\n",
      "Mean Square Error for DecisionTreeRegressor() is:  1.1960754602282024\n",
      "Mean Square Error for DecisionTreeRegressor() after cross validation is:  [1.11076202 1.45642333 1.14555344 1.17634284 1.32125916]\n"
     ]
    }
   ],
   "source": [
    "dt = models(tree.DecisionTreeRegressor(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691706b",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0af13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for Ridge(alpha=0.9) is:  0.41045168257446313\n",
      "Pearson's r for Ridge(alpha=0.9) after cross validation is:  [0.28939259 0.31244344 0.29504246 0.34804303 0.36689302]\n",
      "Mean Square Error for Ridge(alpha=0.9) is:  0.6613361135725191\n",
      "Mean Square Error for Ridge(alpha=0.9) after cross validation is:  [0.66303318 0.7829483  0.66233566 0.67191569 0.78456122]\n"
     ]
    }
   ],
   "source": [
    "ridge = models(linear_model.Ridge(alpha=0.9), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e537af",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52ae765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for RandomForestRegressor(random_state=42) is:  0.3438763305236285\n",
      "Pearson's r for RandomForestRegressor(random_state=42) after cross validation is:  [0.24358303 0.21107175 0.26506125 0.33971407 0.33833686]\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) is:  0.7570275702431948\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) after cross validation is:  [0.81503506 1.01898314 0.83333417 0.77170308 0.94353945]\n"
     ]
    }
   ],
   "source": [
    "rf = models(RandomForestRegressor(random_state=42), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5012323",
   "metadata": {},
   "source": [
    "# Evaluation on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228c4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label    language\n",
      "0                                             @user ì•„..    3.0      Korean\n",
      "1                                @user @user je rÃªve ??    2.2      French\n",
      "2                                          thank u, nxt    1.0     English\n",
      "3                            @user ma che cosa HO FATTO    2.6     Italian\n",
      "4      åœ¨æ•™å®¤æ‰“é£æœºï¼Œç«™ç´¯äº†ï¼Œå°±æ¬å‡³å­åä¸‹ï¼Œç«™ç€åç€éƒ½æ˜¯ä½ çˆ¸çˆ¸ï¼Œå¬çˆ¸çˆ¸çš„è¯å“¦ï¼éªšè´§ï¼Œçˆ¸çˆ¸çˆ±ä½ ã€‚ http    3.0     Chinese\n",
      "...                                                 ...    ...         ...\n",
      "3876  @user NÃ£o sei se ele vai terminar com a Kyra (...    3.0  Portuguese\n",
      "3877  @user Coitada...fraquinha....Povo precisa sabe...    1.4  Portuguese\n",
      "3878  @user Ø£ÙŠ Ø¹Ø·Ø± Ø­Ø¨ÙŠØ¨ÙŠ .... Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ø´Ø§Ø± Ø§Ù„Ø¨Ø¹Ø«ÙŠ Ø¹Ø·Ø± ØŸØŸ    3.5      Arabic\n",
      "3879  TSC promoveert naar tweede klasse, ontgoocheli...    1.0       Dutch\n",
      "3880  @user if you pull up to emmas house bald tomor...    1.8     English\n",
      "\n",
      "[3881 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d3b896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "#test_data['text'] = test_data['text'].apply(clean)\n",
    "\n",
    "# Tokenizing and Lemmatization\n",
    "test_data['text'] = test_data['text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "#Creating datasets by language\n",
    "english_test = test_data[test_data['language'] == 'English'] \n",
    "chinese_test = test_data[test_data['language'] == 'Chinese'] \n",
    "french_test = test_data[test_data['language'] == 'French'] \n",
    "italian_test = test_data[test_data['language'] == 'Italian'] \n",
    "portuguese_test = test_data[test_data['language'] == 'Portuguese'] \n",
    "spanish_test = test_data[test_data['language'] == 'Spanish'] \n",
    "arabic_test = test_data[test_data['language'] == 'Arabic'] \n",
    "dutch_test = test_data[test_data['language'] == 'Dutch'] \n",
    "hindi_test = test_data[test_data['language'] == 'Hindi'] \n",
    "korean_test = test_data[test_data['language'] == 'Korean'] \n",
    "\n",
    "# Removing Stopwords\n",
    "#english_test['text'] = english_test['text'].apply(remove_stopwords_english)\n",
    "#chinese_test['text'] = chinese_test['text'].apply(remove_stopwords_chinese)\n",
    "#french_test['text'] = french_test['text'].apply(remove_stopwords_french)\n",
    "#italian_test['text'] = italian_test['text'].apply(remove_stopwords_italian)\n",
    "#portuguese_test['text'] = portuguese_test['text'].apply(remove_stopwords_portuguese)\n",
    "#spanish_test['text'] = spanish_test['text'].apply(remove_stopwords_spanish)\n",
    "#arabic_test['text'] = arabic_test['text'].apply(remove_stopwords_arabic)\n",
    "#dutch_test['text'] = dutch_test['text'].apply(remove_stopwords_dutch)\n",
    "#hindi_test['text'] = hindi_test['text'].apply(remove_stopwords_hindi)\n",
    "#korean_test['text'] = korean_test['text'].apply(remove_stopwords_korean)\n",
    "\n",
    "#test  = pd.concat([english_test, chinese_test, french_test, italian_test, portuguese_test, spanish_test, arabic_test, dutch_test, hindi_test, korean_test])\n",
    "test = test_data\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "test_X = tfidf_vectorizer.transform(test['text'])\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eae870",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a82fbe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for LinearRegression() is:  0.1466130222052112\n",
      "Pearson's r for LinearRegression() after cross validation is:  [0.20851078 0.16861297 0.18241796 0.22750515 0.19299588]\n",
      "Mean Square Error for LinearRegression() is:  1.4928472459008737\n",
      "Mean Square Error for LinearRegression() after cross validation is:  [1.15868499 1.23329957 1.10916381 1.06707512 1.25833742]\n"
     ]
    }
   ],
   "source": [
    "lr_test = models(linear_model.LinearRegression(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aaa28",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f15efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for SVR() is:  0.2298459746013823\n",
      "Pearson's r for SVR() after cross validation is:  [0.35319267 0.31038057 0.33721598 0.32290689 0.3306959 ]\n",
      "Mean Square Error for SVR() is:  0.9749033682013704\n",
      "Mean Square Error for SVR() after cross validation is:  [0.88968995 0.89137452 0.80110303 0.8175126  0.84275587]\n"
     ]
    }
   ],
   "source": [
    "svr_test = models(svm.SVR(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9f45e",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b54685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for DecisionTreeRegressor() is:  0.18205010322063803\n",
      "Pearson's r for DecisionTreeRegressor() after cross validation is:  [0.15654114 0.16338708 0.21649265 0.19948342 0.19191758]\n",
      "Mean Square Error for DecisionTreeRegressor() is:  1.2375066454803945\n",
      "Mean Square Error for DecisionTreeRegressor() after cross validation is:  [1.2262826  1.25483461 1.13938939 1.14244206 1.19548184]\n"
     ]
    }
   ],
   "source": [
    "dt_test = models(tree.DecisionTreeRegressor(), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09127f37",
   "metadata": {},
   "source": [
    "#### Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8b5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for Ridge(alpha=0.9) is:  0.21784341254799272\n",
      "Pearson's r for Ridge(alpha=0.9) after cross validation is:  [0.29606818 0.27051856 0.29168931 0.28057036 0.28893488]\n",
      "Mean Square Error for Ridge(alpha=0.9) is:  1.018905446864162\n",
      "Mean Square Error for Ridge(alpha=0.9) after cross validation is:  [0.88314438 0.89725691 0.824735   0.83063461 0.85828462]\n"
     ]
    }
   ],
   "source": [
    "ridge_test = models(linear_model.Ridge(alpha=0.9), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd18a9",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a8377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for RandomForestRegressor(random_state=42) is:  0.26227367783835914\n",
      "Pearson's r for RandomForestRegressor(random_state=42) after cross validation is:  [0.28657197 0.27730172 0.28656303 0.30693723 0.22101396]\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) is:  0.9595807549608741\n",
      "Mean Square Error for RandomForestRegressor(random_state=42) after cross validation is:  [0.92961157 0.94451142 0.85728764 0.85166243 0.94838757]\n"
     ]
    }
   ],
   "source": [
    "rf_test = models(RandomForestRegressor(random_state=42), X_train, y_train, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5af4fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_X)\n",
    "    \n",
    "test_with_pred = pd.DataFrame({\n",
    "        'text': test['text'],\n",
    "        'language': test['language'],\n",
    "        'predicted_label': y_pred,\n",
    "        'true_label': test_y\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095dfe99",
   "metadata": {},
   "source": [
    "#### Pearson R for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb99615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation for Korean with SVR is -0.04630847800407837\n",
      "Pearson correlation for Arabic with SVR is 0.1715946218286482\n",
      "Pearson correlation for Portuguese with SVR is 0.44983401762435216\n",
      "Pearson correlation for English with SVR is 0.40963944670035474\n",
      "Pearson correlation for French with SVR is 0.45503915202838935\n",
      "Pearson correlation for Spanish with SVR is 0.5242129956623359\n",
      "Pearson correlation for Italian with SVR is 0.416692357071758\n",
      "Pearson correlation for Hindi with SVR is 0.17567025158467825\n",
      "Pearson correlation for Dutch with SVR is 0.19130633387004328\n",
      "Pearson correlation for Chinese with SVR is -0.005174803024200055\n"
     ]
    }
   ],
   "source": [
    "def calculate_pearsonr_for_diff_lang(language, data):\n",
    "    language_with_pred = test_with_pred[test_with_pred['language'] == language]\n",
    "    pearson, _ = pearsonr(language_with_pred['predicted_label'], language_with_pred['true_label'])\n",
    "    \n",
    "    return pearson\n",
    "\n",
    "# Example usage:\n",
    "languages = set(test_with_pred['language'])\n",
    "\n",
    "for lang in languages:\n",
    "    correlation = calculate_pearsonr_for_diff_lang(lang, test_with_pred)\n",
    "    print(f\"Pearson correlation for {lang} with SVR is {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f13e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
